{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPo7RfTtMFHn0kdZd9lvT+i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Feature Selection (DAE)"],"metadata":{"id":"jE4utr-lbudD"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","class DAE(object):\n","\n","\t\"\"\"\n","\tDenoising autoencoder. Gaussian noise is added. The scale and standard deviation\n","\tof it are noise_scale and noise_std, respectively.\n","\t\"\"\"\n","\n","\tdef __init__(self, n_feature, n_hidden, noise_scale, noise_std, reg_lamda = 0.01):\n","\n","\t\tself.n_hidden = n_hidden\n","\t\tself.n_feature = n_feature\n","\t\tself.reg_lamda = reg_lamda\n","\t\tself.noise_scale = noise_scale\n","\t\tself.noise_std = noise_std\n","\n","\t\tself.data = tf.placeholder(shape = [None, n_feature],\n","\t\t                           dtype = tf.float64)\n","\t\tself.noise = self.noise_scale * tf.random_normal([n_feature], dtype = tf.float64,\n","\t\t                                               stddev = self.noise_std)\n","\t\tdata_with_noise = self.data + self.noise\n","\n","\t\tself.weight_encoder = tf.get_variable(name = 'weight_encoder',\n","\t\t\t                    shape = [self.n_feature, self.n_hidden],\n","\t\t\t                    dtype = tf.float64)\n","\t\tself.bias_encoder = tf.Variable(tf.zeros([self.n_hidden],\n","\t\t\t                                      dtype = tf.float64),\n","\t\t                                name = 'bias_encoder')\n","\n","\t\tweight_decoder = tf.get_variable(name = 'weight_decoder',\n","\t\t\t                    shape = [self.n_hidden, self.n_feature],\n","\t\t\t                    dtype = tf.float64)\n","\t\tbias_decoder = tf.Variable(tf.zeros([self.n_feature], dtype = tf.float64),\n","\t\t\t                           name = 'bias_decoder')\n","\n","\t\twith tf.name_scope('Encoder'):\n","\t\t\tdata_encoded = tf.add(tf.matmul(data_with_noise, self.weight_encoder),\n","\t\t\t\t                  self.bias_encoder)\n","\t\t\tdata_encoded = tf.nn.tanh(data_encoded)\n","\n","\t\twith tf.name_scope('Decoder'):\n","\t\t\tdata_recons = tf.add(tf.matmul(data_encoded, weight_decoder),\n","\t\t\t\t                 bias_decoder)\n","\t\t\tself.data_recons = tf.tanh(data_recons)\n","\n","\t\twith tf.name_scope('Loss'):\n","\t\t\tdiff = self.data_recons - self.data\n","\t\t\tself.loss_mse = 0.5 * tf.reduce_mean(tf.reduce_sum(diff**2, axis = 1))\n","\t\t\tloss_reg = tf.reduce_sum(tf.sqrt(tf.reduce_sum(self.weight_encoder ** 2, axis = 1)))\n","\t\t\tself.loss_reg = self.reg_lamda * loss_reg\n","\t\t\tself.l2_loss = tf.nn.l2_loss(weight_decoder) * 1E-3\n","\n","\t\t\tself.loss = self.loss_mse + self.loss_reg + self.l2_loss\n","\n","\t\twith tf.name_scope('weight_vector'):\n","\t\t\tself.weight_vector = tf.reduce_sum(self.weight_encoder ** 2, axis = 1)\n","\n","\n","class unbalanced_DAE(object):\n","\n","\t\"\"\"\n","\tAn unbalanced version of DAE. the differences is that a weight pos_weight is added\n","\tto the MSE reconstruction loss for positive examples. For this purpose, the labels\n","\tof the examples are used.\n","\t\"\"\"\n","\n","\tdef __init__(self, n_feature, n_hidden, noise_scale, noise_std,\n","\t             posi_weight = 1.0, reg_lamda = 0.00):\n","\n","\t\tself.n_hidden = n_hidden\n","\t\tself.n_feature = n_feature\n","\t\tself.reg_lamda = reg_lamda\n","\t\tself.noise_scale = noise_scale\n","\t\tself.noise_std = noise_std\n","\t\tself.posi_weight = posi_weight\n","\n","\t\tself.data = tf.placeholder(shape = [None, n_feature], dtype = tf.float64)\n","\t\tself.label = tf.placeholder(shape = [None, 1], dtype = tf.float64)\n","\t\t# self.data = tf.compat.v1.placeholder(shape = [None, n_feature], dtype = tf.float64)\n","\t\t# self.label = tf.compat.v1.placeholder(shape = [None, 1], dtype = tf.float64)\n","\t\t# self.data = tf.random.uniform([None, n_feature], dtype = tf.float64)\n","\t\t# self.label = tf.random.uniform([None, 1], dtype = tf.float64)\n","\t\tself.noise = self.noise_scale * tf.random_normal([n_feature], dtype = tf.float64,\n","\t\t                                                  stddev = self.noise_std)\n","\t\tdata_with_noise = self.data + self.noise\n","\n","\t\tself.weight_encoder = tf.get_variable(name = 'weight_encoder',\n","\t\t\t                    shape = [self.n_feature, self.n_hidden],\n","\t\t\t                    dtype = tf.float64)\n","\t\tself.bias_encoder = tf.Variable(tf.zeros([self.n_hidden],\n","\t\t\t                                      dtype = tf.float64),\n","\t\t                                name = 'bias_encoder')\n","\n","\t\tweight_decoder = tf.get_variable(name = 'weight_decoder',\n","\t\t\t                    shape = [self.n_hidden, self.n_feature],\n","\t\t\t                    dtype = tf.float64)\n","\t\tbias_decoder = tf.Variable(tf.zeros([self.n_feature], dtype = tf.float64),\n","\t\t\t                           name = 'bias_decoder')\n","\n","\t\twith tf.name_scope('Encoder'):\n","\t\t\tdata_encoded = tf.add(tf.matmul(data_with_noise, self.weight_encoder),\n","\t\t\t\t                  self.bias_encoder)\n","\t\t\tdata_encoded = tf.nn.sigmoid(data_encoded)\n","\n","\t\twith tf.name_scope('Decoder'):\n","\t\t\tdata_recons = tf.add(tf.matmul(data_encoded, weight_decoder),\n","\t\t\t\t                 bias_decoder)\n","\t\t\tself.data_recons = tf.nn.sigmoid(data_recons)\n","\n","\t\twith tf.name_scope('Loss'):\n","\t\t\tdiff = self.data_recons - self.data\n","\t\t\tweights = self.label * (posi_weight -1) + 1\n","\t\t\tweights = tf.reshape(weights, shape = [-1])\n","\t\t\tself.loss_mse = 0.5 * tf.reduce_mean(tf.reduce_sum(diff**2, axis = 1) * weights)\n","\t\t\tloss_reg = tf.reduce_sum(tf.sqrt(tf.reduce_sum(self.weight_encoder ** 2, axis = 1)))\n","\t\t\tself.loss_reg = self.reg_lamda * loss_reg\n","\t\t\tself.l2_loss = tf.nn.l2_loss(weight_decoder) * 1E-3\n","\n","\t\t\tself.loss = self.loss_mse + self.loss_reg + self.l2_loss\n","\n","\t\twith tf.name_scope('weight_vector'):\n","\t\t\tself.weight_vector = tf.reduce_sum(self.weight_encoder ** 2, axis = 1)"],"metadata":{"id":"e3ojKl28b2pC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","def P_R_F1(confusion_matrix):\n","\n","    category = confusion_matrix.shape[0]\n","    precision = []\n","    recall = []\n","    f1 = []\n","    for i in range (category):\n","        TP = confusion_matrix[i,i]\n","\n","        precsion_temp = TP/np.sum(confusion_matrix[:,i])\n","        recall_temp = TP/np.sum(confusion_matrix[i,:])\n","        f1_temp = 2*precsion_temp*recall_temp/(precsion_temp + recall_temp)\n","\n","        precision.append(precsion_temp)\n","        recall.append(recall_temp)\n","        f1.append(f1_temp)\n","\n","    return precision, recall, f1\n","\n","# shaping labels to one-hot vectors for trainning\n","def label_coding(label, batch_size, category):\n","    new_label = tf.cast(label, dtype = tf.int32)\n","    new_label = tf.reshape(new_label, [batch_size, 1])\n","    new_label = tf.one_hot(new_label, depth = category)\n","    return tf.reshape(new_label, [batch_size, category])\n","\n","# get next batch of data and label\n","def next_batch(filename, batch_size, conf, buffer_size = 0):\n","\n","    len_feature = conf.len_feature\n","    len_label = conf.len_label\n","    num_classes = conf.num_classes\n","    one_hot_encoding = conf.one_hot_encoding\n","\n","    def read_data(examples):\n","        features = {\"features\": tf.FixedLenFeature([len_feature], tf.float32),\n","                    \"label_2\": tf.FixedLenFeature([len_label], tf.float32),\n","                    \"label_10\": tf.FixedLenFeature([len_label], tf.float32)}\n","        parsed_features = tf.parse_single_example(examples, features)\n","        return parsed_features['features'], parsed_features['label_2'], \\\n","               parsed_features['label_10']\n","\n","    data = tf.data.TFRecordDataset(filename)\n","    data = data.map(read_data)\n","    if buffer_size != 0:\n","        data = data.shuffle(buffer_size = buffer_size)\n","    data = data.repeat()\n","    data = data.batch(batch_size)\n","    iterator = data.make_one_shot_iterator()\n","    next_data, next_label_2, next_label_10 = iterator.get_next()\n","\n","    if one_hot_encoding == True:\n","        if num_classes == 2:\n","            next_label_2 = label_coding(next_label_2, batch_size,\n","                                        num_classes)\n","        else:\n","            next_label_10 = label_coding(next_label_10, batch_size,\n","                                         num_classes)\n","\n","    return next_data, next_label_2, next_label_10\n","\n","\n","def trans_dataset(file_tfr, file_txt, num_examples, num_classes):\n","    # with tf.Session() as sess:\n","    all_data, all_label = next_batch(file_tfr, num_examples)\n","    all_label = label_coding(all_label, num_examples, num_classes)\n","\n","    # record = np.concatenate([sess.run(all_data), sess.run(all_label)], axis = 1)\n","    record = np.concatenate([all_data, all_label], axis = 1)\n","    np.savetxt(file_txt, record, fmt = '%.6e')\n","\n","def split_dataset(file_train, file_test, k, file_folder_new): # k is refer to k_fold\n","\n","    trainset = np.loadtxt(file_train)\n","    testset = np.loadtxt(file_test)\n","    dataset = np.concatenate((trainset, testset))\n","\n","    for i in range(k - 1):\n","        trainset, testset = train_test_split(dataset, test_size = 1/(k - i))\n","        dataset = trainset\n","        np.savetxt(file_folder_new + str(i) + '.txt', testset)\n","\n","    np.savetxt(file_folder_new + str(k - 1) + '.txt', trainset)\n","\n","def get_dataset(file_folder, index_test, indices_train):\n","\n","    testset = np.loadtxt(file_folder + str(index_test) + '.txt')\n","\n","    count = 0\n","    for other in indices_train:\n","        temp = np.loadtxt(file_folder + str(other) + '.txt')\n","\n","        if count == 0:\n","            trainset = temp\n","        else:\n","            trainset = np.concatenate((trainset, temp))\n","\n","        count += 1\n","\n","    return trainset, testset\n","\n","def parse_pos_neg(dataset):\n","\n","    label = dataset[:, -1]\n","\n","    record_posi = []\n","    record_neg = []\n","\n","    records_len = dataset.shape[-1]\n","    records_num = dataset.shape[0]\n","\n","    for index in range(records_num):\n","        record = dataset[index, :]\n","        record = np.reshape(record, (1, records_len))\n","        if label[index] == 0.:\n","            record_posi.append(record)\n","        else:\n","            record_neg.append(record)\n","\n","    posi = np.concatenate(record_posi)\n","    neg = np.concatenate(record_neg)\n","\n","    return posi, neg\n","\n","\"\"\"\n","if __name__ == '__main__':\n","\n","    num_train = 125973\n","    num_test = 22543\n","    file_folder = 'normalized/'\n","\n","    num_classes = 2\n","    trans_dataset(file_folder + 'train+.tfrecords', file_folder + 'train2.txt', num_train, num_classes)\n","    trans_dataset(file_folder + 'test+.tfrecords', file_folder + 'test2.txt', num_test, num_classes)\n","\n","    num_classes = 5\n","    trans_dataset(file_folder + 'train5.tfrecords', file_folder + 'train5.txt', num_train, num_classes)\n","    trans_dataset(file_folder + 'test5.tfrecords', file_folder + 'test5.txt', num_test, num_classes)\n","\n","    file_folder_new = file_folder + 'cross_validation_5/'\n","\n","    file_train = file_folder + 'train5.txt'\n","    file_test = file_folder +'test5.txt'\n","    split_dataset(file_train, file_test, k =10, file_folder_new = file_folder_new)\n","\n","    dataset = np.loadtxt(file_folder +'train_new.txt')\n","    #dataset = [file_folder + str(x + 1) + '_train.csv' for x in range(4)]\n","    posi, neg = parse_pos_neg(dataset)\n","\n","    np.savetxt(file_folder + 'train_posi.txt',posi )\n","    np.savetxt(file_folder + 'train_neg.txt', neg)\n","    \"\"\""],"metadata":{"id":"Yj16BTI6b75X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","# import tensorflow as tf\n","import tensorflow.compat.v1 as tf\n","import numpy as np\n","#from autoencoders import DAE, unbalanced_DAE\n","#from utils import next_batch, P_R_F1\n","\n","tf.disable_v2_behavior()\n","tf.reset_default_graph()\n","\n","# system parameters\n","\n","class Configures(object):\n","\n","\tdef __init__(self):\n","        # parameter of records\n","\t\tself.len_feature = 202\n","\t\tself.len_label = 1\n","\t\t#self.num_classes = 2\n","\t\tself.num_classes = 10\n","\t\tself.one_hot_encoding = False\n","\t\tself.num_records_train = 1625628\n","\t\tself.num_records_test = 508012\n","\n","        # parameters for training\n","\t\tself.batch_size = 256\n","\t\tself.batch_size_test = 2048\n","\t\tself.training_epochs = 2\n","\t\tself.learn_rate_start = 1E-4\n","\n","\t\tself.batch_train = self.num_records_train//self.batch_size\n","\t\tself.batch_test = self.num_records_test//self.batch_size_test\n","\n","n_hidden = 64\n","noise_scale = 0.\n","noise_std = 0.1\n","conf = Configures()\n","\n","# training op\n","with tf.Session() as sess:\n","\n","\t#AE = DAE(conf.len_feature, n_hidden, noise_scale, noise_std, reg_lamda = 0.001)\n","\tAE = unbalanced_DAE(conf.len_feature, n_hidden, noise_scale, noise_std, posi_weight = 3.5,\n","\t\t                reg_lamda = 0.001)\n","\n","\tglobal_step = tf.Variable(0, name = 'training_steps', trainable = False)\n","\tlearn_rate = tf.train.exponential_decay(conf.learn_rate_start, global_step, 2000, 0.96, staircase=True)\n","\tupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","\twith tf.control_dependencies(update_ops):\n","\t\toptimizer = tf.train.AdamOptimizer(conf.learn_rate_start)\n","\t\tgrads_and_vars = optimizer.compute_gradients(AE.loss)\n","\t\tgrads_clipped = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in grads_and_vars]\n","\t\ttrain_op = optimizer.apply_gradients(grads_clipped, global_step = global_step)\n","\n","\tsess.run(tf.global_variables_initializer())\n","\n","\t# Reading data\n","\tfile_train = ['/content/drive/MyDrive/Colab Notebooks/UNSW-NB15 - CSV Files/normalized/train_202.tfrecords',\n","               '/content/drive/MyDrive/Colab Notebooks/UNSW-NB15 - CSV Files/normalized/validation_202.tfrecords']\n","\tfile_test = '/content/drive/MyDrive/Colab Notebooks/UNSW-NB15 - CSV Files/normalized/test_202.tfrecords'\n","  # file_train = ['/content/normalized/train.tfrecords', '/content/normalized/validation.tfrecords']\n","\t# file_test = '/content/normalized/test.tfrecords'\n","\ttrain_data, train_label, _ = next_batch(file_train, conf.batch_size, conf, 150)\n","\ttest_data, test_label, _ = next_batch(file_test, conf.batch_size_test, conf)\n","\n","\tmin_loss = 100.\n","\tfor epoch in range(conf.training_epochs):\n","\t\ttime_start = time.time()\n","\t\ttotal_mse_loss = 0.\n","\t\ttotal_loss = 0.\n","\t\ttotal_reg_loss = 0.\n","\n","\t\tfor step in range(conf.batch_train):\n","\t\t\tdata, label = sess.run([train_data, train_label])\n","\n","\t\t\tfeed_dict = {AE.data: data, AE.label: label}\n","\t\t\t_, loss, loss_mse, loss_reg = sess.run([train_op, AE.loss, AE.loss_mse, AE.loss_reg],\n","\t\t\t                                        feed_dict = feed_dict)\n","\n","\t\t\ttotal_loss += loss\n","\t\t\ttotal_mse_loss += loss_mse\n","\t\t\ttotal_reg_loss += loss_reg\n","\n","\t\ttime_train_end = time.time()\n","\t\ttest_loss = 0.\n","\t\ttest_loss_reg = 0.\n","\t\ttest_loss_mse = 0.\n","\n","\t\tfor step in range(conf.batch_test):\n","\t\t\tdata, label = sess.run([test_data, test_label])\n","\n","\t\t\tfeed_dict = {AE.data: data, AE.label: label}\n","\t\t\tweights, loss, loss_mse, loss_reg = sess.run([AE.weight_vector, AE.loss, AE.loss_mse, AE.loss_reg],\n","\t\t\t                                              feed_dict = feed_dict)\n","\t\t\ttest_loss += loss/conf.batch_test\n","\t\t\ttest_loss_mse += loss_mse/conf.batch_test\n","\t\t\ttest_loss_reg += loss_reg/conf.batch_test\n","\n","\t\ttime_test_end = time.time()\n","\n","\t\ttime_duration_train = int(time_train_end - time_start)\n","\t\ttime_duration_test = int(time_test_end - time_train_end)\n","\n","\n","\t\tif test_loss < min_loss:\n","\t\t\tmin_loss = test_loss\n","\n","\t\t\tfile_folder = 'feature_select_AE/'\n","\t\t\tif not os.path.exists(file_folder):\n","\t\t\t\tos.mkdir(file_folder)\n","\n","\t\t\tfile_path = os.path.join(file_folder, 'weights_new_3.5.txt')\n","\t\t\tif not os.path.exists(file_path):\n","\t\t\t\twith open(file_path, 'x'):\n","\t\t\t\t\tpass  # Just create the file and do nothing else\n","\n","\t\t\tnp.savetxt('feature_select_AE/weights_new_3.5.txt', weights, fmt = \"%.6E\")\n","\n","\t\tprint(\"Epoch:\", \"%d,\" % (epoch + 1),\n","\t\t\t  \"Loss on Train:\", \"{:.6f}\".format(total_loss/(conf.batch_train)),\n","\t\t\t  \"mse on Train:\", \"{:.6f}\".format(total_mse_loss/conf.batch_train),\n","\t\t\t  \"reg on Train:\", \"{:.6f}\".format(total_reg_loss/conf.batch_train),\n","\t\t\t  \"Loss on Test:\", \"{:.6f}\".format(test_loss),\n","\t\t\t  \"mse on Test:\", \"{:.6f}\".format(test_loss_mse),\n","\t\t\t  \"reg on Test:\", \"{:.6f}\".format(test_loss_reg))\n","\n","\t\tprint(time_duration_train)\n","\t\tprint(time_duration_test)"],"metadata":{"id":"NK241BlVcBP6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import heapq\n","import os\n","import re\n","import matplotlib.pyplot as plt\n","import numpy as np\n","# import tensorflow as tf\n","import tensorflow.compat.v1 as tf\n","from pandas import read_csv\n","\n","tf.disable_v2_behavior()\n","\n","def get_indices(num_select, num_feature, file_weights):\n","\t\"\"\"\n","\tThis function is to select maximum k features according to their\n","\tweights.\n","\n","\tPram:\n","\t\tnum_select: An interger, the number of the selected features\n","\t\tnum_feature: An interger, the number of the original features\n","\t\tfile_weights: A txt file storing a numpy array. Each row of the\n","\t\t              array is the weight for a feature\n","\tReturn:\n","\t\ta list containing the indices of selected features\n","\t\"\"\"\n","\n","\tx = np.arange(1, num_feature + 1)\n","\ty = np.loadtxt(file_weights)\n","\tindices = heapq.nlargest(num_select, range(len(y)), y.take)\n","\tplt.scatter(x, y)\n","\tplt.show()\n","\tplt.savefig('weights_dis.eps', format = 'eps')\n","\tprint(indices)\n","\n","\treturn indices\n","\n","\n","def read_data(examples):\n","    features = {\"features\": tf.FixedLenFeature([num_feature], tf.float32),\n","                \"label_2\": tf.FixedLenFeature([len_label], tf.float32),\n","                \"label_10\": tf.FixedLenFeature([len_label], tf.float32)}\n","    parsed_features = tf.parse_single_example(examples, features)\n","    return parsed_features['features'], parsed_features['label_2'], \\\n","           parsed_features['label_10']\n","\n","\n","# get next batch of data and label\n","def next_batch(filename, num_examples):\n","\n","    data = tf.data.TFRecordDataset(filename)\n","    data = data.map(read_data)\n","    data = data.batch(num_examples)\n","    iterator = data.make_one_shot_iterator()\n","    next_data, next_label_2, next_label_10 = iterator.get_next()\n","    return next_data, next_label_2, next_label_10\n","\n","\n","def make_tfrecords(dataset, file_to_save):\n","\t[features, label_2, label_10] = dataset\n","\n","\twith tf.python_io.TFRecordWriter(file_to_save) as writer:\n","\t\tfor index in range(features.shape[0]):\n","\t\t\tfeature = {'features': tf.train.Feature(float_list = tf.train.FloatList(value = features[index, :])),\n","\t\t\t           'label_2': tf.train.Feature(float_list = tf.train.FloatList(value = label_2[index, :])),\n","\t\t\t           'label_10': tf.train.Feature(float_list = tf.train.FloatList(value = label_10[index, :]))}\n","\t\t\texample = tf.train.Example(features = tf.train.Features(feature = feature))\n","\t\t\twriter.write(example.SerializeToString())\n","\n","\n","def selection(data, indices):\n","\t\"\"\"\n","\tselect the columns (indicating the features) according to the indices\n","\t\"\"\"\n","\n","\treturn data[:, indices]\n","\n","\n","def select_feature(file, num_examples, indices):\n","\t\"\"\"\n","\tThe main function of feature selection.\n","\n","\tParams:\n","\t  file: The .tfrecords file containing original data.tfrecords\n","\t  num_examples: The number of examples in the file\n","\t  indices: The indices of features to be selected\n","\n","\tReturn:\n","\t  None\n","\t  In the function, a new .tfrecords file with tail of 'selected'\n","\t  will be created in the same folder with the original data\n","\t\"\"\"\n","\n","\twith tf.Session() as sess:\n","\t\tdata, label_2, label_10 = sess.run(next_batch(file, num_examples))\n","\n","\tdata_select = selection(data, indices)\n","\n","\tfile_name = file.split('\\\\')[-1]\n","\tfile_tail = len('.tfrecords')\n","\tfile_to_save = file_name[:-1*file_tail] + '_select_' + str(len(indices)) + '.tfrecords'\n","\n","\n","\tmake_tfrecords([data_select, label_2, label_10], file_to_save)\n","\n","\n","def show_feature_name(indices):\n","\t\"\"\"\n","\tThe function to convert indices to feature names\n","\n","\tParams:\n","\t  indices:the indices of the features\n","\tReturn:\n","\t  None.\n","\t  The name of features will be print\n","\t\"\"\"\n","\n","\t#dirname = os.path.dirname(os.getcwd())\n","\t#file = os.path.join(dirname, 'normalized/', '1_test.csv')\n","\tfile = os.path.join( '/content/drive/MyDrive/Colab Notebooks/UNSW-NB15 - CSV Files/normalized/', '1_test.csv')\n","\t# file = os.path.join( 'normalized/', '1_test.csv')\n","\tdata = read_csv(file, index_col = 0)\n","\tcols = data.columns\n","\n","\tfor x in indices:\n","\t\tprint(cols[x])\n","\n","\n","if __name__ == '__main__':\n","\n","\tnum_select = 12   #The number of selected features\n","\tnum_feature = 202\n","\tlen_label = 1\n","\tfile_weights = '/content/drive/MyDrive/Colab Notebooks/UNSW-NB15 - CSV Files/feature_select_AE/weights_new_3.5.txt'\n","\t# file_weights = 'feature_select_AE/weights_new_3.5.txt'\n","\n","\tindices = get_indices(num_select, num_feature, file_weights)\n","\t#print(indices)\n","\tshow_feature_name(indices)\n","\n","\tdirname = os.path.dirname(os.getcwd())\n","\tfile_folder = \"/content/drive/MyDrive/Colab Notebooks/UNSW-NB15 - CSV Files/normalized\"\n","\t# file_folder = os.path.join(dirname, '/content/', 'normalized/')\n","\n","\tfile_train = file_folder + 'train_202.tfrecords'\n","\tfile_valid = file_folder + 'validation_202.tfrecords'\n","\tnum_examples_train = 1778030\n","\tnum_examples_validation = 254005\n","\n","\tfile_test = file_folder + 'test_202.tfrecords'\n","\tnum_examples_test = 508012\n","\n","\n","\tselect_feature(file_train, num_examples_train, indices)\n","\tselect_feature(file_valid, num_examples_validation, indices)\n","\tselect_feature(file_test, num_examples_test, indices)"],"metadata":{"id":"mD_WhtZDcJMn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_indices(num_select, num_feature, file_weights)"],"metadata":{"id":"AvLbl6KqcKO3"},"execution_count":null,"outputs":[]}]}